usage: polygraphy run [-h] [-v] [-q] [--silent]
                      [--log-format {timestamp,line-info,no-colors} [{timestamp,line-info,no-colors} ...]]
                      [--log-file LOG_FILE]
                      [--model-type {frozen,keras,ckpt,onnx,engine,uff,trt-network-script,caffe}]
                      [--input-shapes INPUT_SHAPES [INPUT_SHAPES ...]]
                      [--ckpt CKPT] [--tf-outputs TF_OUTPUTS [TF_OUTPUTS ...]]
                      [--save-pb SAVE_PB]
                      [--save-tensorboard SAVE_TENSORBOARD] [--freeze-graph]
                      [--tftrt] [--minimum-segment-size MINIMUM_SEGMENT_SIZE]
                      [--dynamic-op]
                      [--gpu-memory-fraction GPU_MEMORY_FRACTION]
                      [--allow-growth] [--xla] [--save-timeline SAVE_TIMELINE]
                      [--opset OPSET] [--no-const-folding]
                      [--save-onnx SAVE_ONNX]
                      [--save-external-data [SAVE_EXTERNAL_DATA]]
                      [--external-data-size-threshold EXTERNAL_DATA_SIZE_THRESHOLD]
                      [--no-save-all-tensors-to-one-file] [--shape-inference]
                      [--external-data-dir LOAD_EXTERNAL_DATA]
                      [--onnx-outputs ONNX_OUTPUTS [ONNX_OUTPUTS ...]]
                      [--onnx-exclude-outputs ONNX_EXCLUDE_OUTPUTS [ONNX_EXCLUDE_OUTPUTS ...]]
                      [--trt-min-shapes TRT_MIN_SHAPES [TRT_MIN_SHAPES ...]]
                      [--trt-opt-shapes TRT_OPT_SHAPES [TRT_OPT_SHAPES ...]]
                      [--trt-max-shapes TRT_MAX_SHAPES [TRT_MAX_SHAPES ...]]
                      [--tf32] [--fp16] [--int8]
                      [--obey-precision-constraints] [--strict-types]
                      [--sparse-weights] [--workspace BYTES]
                      [--calibration-cache CALIBRATION_CACHE]
                      [--calib-base-cls CALIBRATION_BASE_CLASS]
                      [--quantile QUANTILE]
                      [--regression-cutoff REGRESSION_CUTOFF]
                      [--timing-cache TIMING_CACHE]
                      [--tactic-replay TACTIC_REPLAY | --save-tactics SAVE_TACTICS | --load-tactics LOAD_TACTICS]
                      [--tactic-sources [TACTIC_SOURCES [TACTIC_SOURCES ...]]]
                      [--trt-config-script TRT_CONFIG_SCRIPT]
                      [--trt-config-func-name TRT_CONFIG_FUNC_NAME]
                      [--trt-safety-restricted] [--use-dla]
                      [--allow-gpu-fallback] [--plugins PLUGINS [PLUGINS ...]]
                      [--explicit-precision]
                      [--trt-outputs TRT_OUTPUTS [TRT_OUTPUTS ...]]
                      [--trt-exclude-outputs TRT_EXCLUDE_OUTPUTS [TRT_EXCLUDE_OUTPUTS ...]]
                      [--trt-network-func-name TRT_NETWORK_FUNC_NAME]
                      [--save-engine SAVE_ENGINE] [-p PREPROCESSOR]
                      [--uff-order UFF_ORDER] [--batch-size SIZE]
                      [--model CAFFE_MODEL] [--save-uff] [--seed SEED]
                      [--val-range VAL_RANGE [VAL_RANGE ...]]
                      [--int-min INT_MIN] [--int-max INT_MAX]
                      [--float-min FLOAT_MIN] [--float-max FLOAT_MAX]
                      [--iterations NUM]
                      [--load-inputs LOAD_INPUTS [LOAD_INPUTS ...]]
                      [--data-loader-script DATA_LOADER_SCRIPT]
                      [--data-loader-func-name DATA_LOADER_FUNC_NAME]
                      [--warm-up NUM] [--use-subprocess]
                      [--save-inputs SAVE_INPUTS]
                      [--save-outputs SAVE_RESULTS] [--no-shape-check]
                      [--rtol RTOL [RTOL ...]] [--atol ATOL [ATOL ...]]
                      [--validate] [--fail-fast] [--top-k TOP_K [TOP_K ...]]
                      [--check-error-stat CHECK_ERROR_STAT [CHECK_ERROR_STAT ...]]
                      [--load-outputs LOAD_RESULTS [LOAD_RESULTS ...]]
                      [--gen GEN_SCRIPT] [--trt] [--trt-legacy] [--tf]
                      [--onnxrt] [--pluginref]
                      [model_file]

Run inference and compare results across backends.

The typical usage of `run` is:

    polygraphy run [model_file] [runners...] [runner_options...]

`run` will then run inference on the specified model with all the specified runners
and compare inference outputs between them.

TIP: You can use `--gen-script` to generate a Python script that does exactly what the `run`
command would otherwise do.

optional arguments:
  -h, --help            show this help message and exit
  --gen GEN_SCRIPT, --gen-script GEN_SCRIPT
                        Path to save a generated Python script, that will do
                        exactly what `run` would. When this option is enabled,
                        `run` will just save the script and exit. Use `-` to
                        print the script to the standard output

Logging:
  Options for logging and debug output

  -v, --verbose         Increase logging verbosity. Specify multiple times for
                        higher verbosity
  -q, --quiet           Decrease logging verbosity. Specify multiple times for
                        lower verbosity
  --silent              Disable all output
  --log-format {timestamp,line-info,no-colors} [{timestamp,line-info,no-colors} ...]
                        Format for log messages: {{'timestamp': Include
                        timestamp, 'line-info': Include file and line number,
                        'no-colors': Disable colors}}
  --log-file LOG_FILE   Path to a file where Polygraphy logging output should
                        be written. This will not include logging output from
                        dependencies, like TensorRT or ONNX-Runtime.

Model:
  Options for the model

  model_file            Path to the model
  --model-type {frozen,keras,ckpt,onnx,engine,uff,trt-network-script,caffe}
                        The type of the input model: {{'frozen': TensorFlow
                        frozen graph, 'keras': Keras model, 'ckpt': TensorFlow
                        checkpoint directory, 'onnx': ONNX model, 'engine':
                        TensorRT engine, 'trt-network-script': A Python script
                        that defines a `load_network` function that takes no
                        arguments and returns a TensorRT Builder, Network, and
                        optionally Parser, 'uff': UFF file [deprecated],
                        'caffe': Caffe prototxt [deprecated]}}
  --input-shapes INPUT_SHAPES [INPUT_SHAPES ...], --inputs INPUT_SHAPES [INPUT_SHAPES ...]
                        Model input(s) and their shape(s). Used to determine
                        shapes to use while generating input data for
                        inference. Format: --input-shapes <name>:<shape>. For
                        example: --input-shapes image:[1,3,224,224]
                        other_input:[10]

TensorFlow Loader:
  Options for TensorFlow Loader

  --ckpt CKPT           [EXPERIMENTAL] Name of the checkpoint to load.
                        Required if the `checkpoint` file is missing. Should
                        not include file extension (e.g. to load `model.meta`
                        use `--ckpt=model`)
  --tf-outputs TF_OUTPUTS [TF_OUTPUTS ...]
                        Name(s) of TensorFlow output(s). Using '--tf-outputs
                        mark all' indicates that all tensors should be used as
                        outputs
  --save-pb SAVE_PB     Path to save the TensorFlow frozen graphdef
  --save-tensorboard SAVE_TENSORBOARD
                        [EXPERIMENTAL] Path to save a TensorBoard
                        visualization
  --freeze-graph        [EXPERIMENTAL] Attempt to freeze the graph

TensorFlow-TensorRT:
  [UNTESTED] Options for TensorFlow-TensorRT Integration

  --tftrt, --use-tftrt  [UNTESTED] Enable TF-TRT integration
  --minimum-segment-size MINIMUM_SEGMENT_SIZE
                        Minimum length of a segment to convert to TensorRT
  --dynamic-op          Enable dynamic mode (defers engine build until
                        runtime)

TensorFlow Session Configuration:
  Options for the TensorFlow Session Configuration

  --gpu-memory-fraction GPU_MEMORY_FRACTION
                        Maximum percentage of GPU memory TensorFlow can
                        allocate per process
  --allow-growth        Allow GPU memory allocated by TensorFlow to grow
  --xla                 [EXPERIMENTAL] Attempt to run graph with xla

TensorFlow Runner:
  Options for TensorFlow Inference

  --save-timeline SAVE_TIMELINE
                        [EXPERIMENTAL] Directory to save timeline JSON files
                        for profiling inference (view at chrome://tracing)

TensorFlow-ONNX Loader:
  Options for TensorFlow-ONNX conversion

  --opset OPSET         Opset to use when converting to ONNX
  --no-const-folding    Do not fold constants in the TensorFlow graph prior to
                        conversion

ONNX Save Options:
  Options for saving ONNX models

  --save-onnx SAVE_ONNX, --save-onnx SAVE_ONNX
                        Path to save the ONNX model
  --save-external-data [SAVE_EXTERNAL_DATA]
                        Whether to save weight data in external file(s). To
                        use a non-default path, supply the desired path as an
                        argument. This is always a relative path; external
                        data is always written to the same directory as the
                        model.
  --external-data-size-threshold EXTERNAL_DATA_SIZE_THRESHOLD
                        The size threshold, in bytes, above which tensor data
                        will be stored in the external file. Tensors smaller
                        that this threshold will remain in the .onnx file.
                        Optionally, use a `K`, `M`, or `G` suffix to indicate
                        KiB, MiB, or GiB respectively. For example,
                        `--external-data-size-threshold=16M` is equivalent to
                        `--external-data-size-threshold=16777216`. Has no
                        effect if `--save-external-data` is not set.
  --no-save-all-tensors-to-one-file
                        Do not save all tensors to a single file when saving
                        external data. Has no effect if `--save-external-data`
                        is not set

ONNX Shape Inference:
  Options for ONNX Shape Inference

  --shape-inference     Enable ONNX shape inference when loading the model

ONNX Loader:
  Options for the ONNX Loader

  --external-data-dir LOAD_EXTERNAL_DATA, --load-external-data LOAD_EXTERNAL_DATA, --ext LOAD_EXTERNAL_DATA
                        Path to a directory containing external data for the
                        model. Generally, this is only required if the
                        external data is not stored in the model directory.
  --onnx-outputs ONNX_OUTPUTS [ONNX_OUTPUTS ...]
                        Name(s) of ONNX tensor(s) to mark as output(s). Using
                        the special value 'mark all' indicates that all
                        tensors should be used as outputs
  --onnx-exclude-outputs ONNX_EXCLUDE_OUTPUTS [ONNX_EXCLUDE_OUTPUTS ...]
                        [EXPERIMENTAL] Name(s) of ONNX output(s) to unmark as
                        outputs.

TensorRT Builder Configuration:
  Options for TensorRT Builder Configuration

  --trt-min-shapes TRT_MIN_SHAPES [TRT_MIN_SHAPES ...]
                        The minimum shapes the optimization profile(s) will
                        support. Specify this option once for each profile. If
                        not provided, inference-time input shapes are used.
                        Format: --trt-min-shapes <input0>:[D0,D1,..,DN] ..
                        <inputN>:[D0,D1,..,DN]
  --trt-opt-shapes TRT_OPT_SHAPES [TRT_OPT_SHAPES ...]
                        The shapes for which the optimization profile(s) will
                        be most performant. Specify this option once for each
                        profile. If not provided, inference-time input shapes
                        are used. Format: --trt-opt-shapes
                        <input0>:[D0,D1,..,DN] .. <inputN>:[D0,D1,..,DN]
  --trt-max-shapes TRT_MAX_SHAPES [TRT_MAX_SHAPES ...]
                        The maximum shapes the optimization profile(s) will
                        support. Specify this option once for each profile. If
                        not provided, inference-time input shapes are used.
                        Format: --trt-max-shapes <input0>:[D0,D1,..,DN] ..
                        <inputN>:[D0,D1,..,DN]
  --tf32                Enable tf32 precision in TensorRT
  --fp16                Enable fp16 precision in TensorRT
  --int8                Enable int8 precision in TensorRT. If calibration is
                        required but no calibration cache is provided, this
                        option will cause TensorRT to run int8 calibration
                        using the Polygraphy data loader to provide
                        calibration data.
  --obey-precision-constraints
                        Enable enforcing precision constraints in TensorRT,
                        forcing it to use tactics based on the layer precision
                        set, even if another precision is faster. Build fails
                        if such an engine cannot be built.
  --strict-types        [DEPRECATED] Enable preference for precision
                        constraints and avoidance of I/O reformatting in
                        TensorRT, and fall back to ignoring the request if
                        such an engine cannot be built.
  --sparse-weights      Enable optimizations for sparse weights in TensorRT
  --workspace BYTES     Amount of memory, in bytes, to allocate for the
                        TensorRT builder's workspace. Optionally, use a `K`,
                        `M`, or `G` suffix to indicate KiB, MiB, or GiB
                        respectively. For example, `--workspace=16M` is
                        equivalent to `--workspace=16777216`.
  --calibration-cache CALIBRATION_CACHE
                        Path to load/save a calibration cache. Used to store
                        calibration scales to speed up the process of int8
                        calibration. If the provided path does not yet exist,
                        int8 calibration scales will be calculated and written
                        to it during engine building. If the provided path
                        does exist, it will be read and int8 calibration will
                        be skipped during engine building.
  --calib-base-cls CALIBRATION_BASE_CLASS, --calibration-base-class CALIBRATION_BASE_CLASS
                        The name of the calibration base class to use. For
                        example, 'IInt8MinMaxCalibrator'.
  --quantile QUANTILE   The quantile to use for IInt8LegacyCalibrator. Has no
                        effect for other calibrator types.
  --regression-cutoff REGRESSION_CUTOFF
                        The regression cutoff to use for
                        IInt8LegacyCalibrator. Has no effect for other
                        calibrator types.
  --timing-cache TIMING_CACHE
                        Path to load/save tactic timing cache. Used to cache
                        tactic timing information to speed up the engine
                        building process. Existing caches will be appended to
                        with any new timing information gathered.
  --tactic-replay TACTIC_REPLAY
                        [DEPRECATED - use --load/save-tactics] Path to
                        load/save a tactic replay file. Used to record and
                        replay tactics selected by TensorRT to provide
                        deterministic engine builds. If the provided path does
                        not yet exist, tactics will be recorded and written to
                        it. If the provided path does exist, it will be read
                        and used to replay previously recorded tactics.
  --save-tactics SAVE_TACTICS
                        Path to save a Polygraphy tactic replay file. Details
                        about tactics selected by TensorRT will be recorded
                        and stored at this location as a JSON file.
  --load-tactics LOAD_TACTICS
                        Path to load a Polygraphy tactic replay file, such as
                        one created by --save-tactics. The tactics specified
                        in the file will be used to override TensorRT's
                        default selections.
  --tactic-sources [TACTIC_SOURCES [TACTIC_SOURCES ...]]
                        Tactic sources to enable. This controls which
                        libraries (e.g. cudnn, cublas, etc.) TensorRT is
                        allowed to load tactics from. Values come from the
                        names of the values in the trt.TacticSource enum, and
                        are case-insensitive. If no arguments are provided,
                        e.g. '--tactic-sources', then all tactic sources are
                        disabled.
  --trt-config-script TRT_CONFIG_SCRIPT
                        Path to a Python script that defines a function that
                        creates a TensorRT IBuilderConfig. The function should
                        take a builder and network as parameters and return a
                        TensorRT builder configuration. When this option is
                        specified, all other config arguments are ignored.
  --trt-config-func-name TRT_CONFIG_FUNC_NAME
                        When using a trt-config-script, this specifies the
                        name of the function that creates the config. Defaults
                        to `load_config`.
  --trt-safety-restricted
                        Enable safety scope checking in TensorRT
  --use-dla             [EXPERIMENTAL] Use DLA as the default device type
  --allow-gpu-fallback  [EXPERIMENTAL] Allow layers unsupported on the DLA to
                        fall back to GPU. Has no effect if --dla is not set.

TensorRT Plugin Loader:
  Options for TensorRT Plugin Loader

  --plugins PLUGINS [PLUGINS ...]
                        Path(s) of plugin libraries to load

TensorRT Network Loader:
  Options for TensorRT Network Loader

  --explicit-precision  Enable explicit precision mode
  --trt-outputs TRT_OUTPUTS [TRT_OUTPUTS ...]
                        Name(s) of TensorRT output(s). Using '--trt-outputs
                        mark all' indicates that all tensors should be used as
                        outputs
  --trt-exclude-outputs TRT_EXCLUDE_OUTPUTS [TRT_EXCLUDE_OUTPUTS ...]
                        [EXPERIMENTAL] Name(s) of TensorRT output(s) to unmark
                        as outputs.
  --trt-network-func-name TRT_NETWORK_FUNC_NAME
                        When using a trt-network-script instead of other model
                        types, this specifies the name of the function that
                        loads the network. Defaults to `load_network`.

TensorRT Engine Save Options:
  Options for saving TensorRT engines

  --save-engine SAVE_ENGINE, --save-engine SAVE_ENGINE
                        Path to save the TensorRT Engine

TensorRT Legacy:
  [DEPRECATED] Options for TensorRT Legacy. Reuses TensorRT options, but does not support int8 mode, or dynamic shapes

  -p PREPROCESSOR, --preprocessor PREPROCESSOR
                        The preprocessor to use for the UFF converter
  --uff-order UFF_ORDER
                        The order of the input
  --batch-size SIZE     The batch size to use in TensorRT when it cannot be
                        automatically determined
  --model CAFFE_MODEL   Model file for Caffe models. The deploy file should be
                        provided as the model_file positional argument
  --save-uff            Save intermediate UFF files

Data Loader:
  Options for controlling how input data is loaded or generated

  --seed SEED           Seed to use for random inputs
  --val-range VAL_RANGE [VAL_RANGE ...]
                        Range of values to generate in the data loader. To
                        specify per-input ranges, use the format: --val-range
                        <input_name>:[min,max]. If no input name is provided,
                        the range is used for any inputs not explicitly
                        specified. For example: --val-range [0,1] inp0:[2,50]
                        inp1:[3.0,4.6]
  --int-min INT_MIN     [DEPRECATED: Use --val-range] Minimum integer value
                        for random integer inputs
  --int-max INT_MAX     [DEPRECATED: Use --val-range] Maximum integer value
                        for random integer inputs
  --float-min FLOAT_MIN
                        [DEPRECATED: Use --val-range] Minimum float value for
                        random float inputs
  --float-max FLOAT_MAX
                        [DEPRECATED: Use --val-range] Maximum float value for
                        random float inputs
  --iterations NUM, --iters NUM
                        Number of inference iterations for which to supply
                        data
  --load-inputs LOAD_INPUTS [LOAD_INPUTS ...], --load-input-data LOAD_INPUTS [LOAD_INPUTS ...]
                        [EXPERIMENTAL] Path(s) to load inputs. The file(s)
                        should be a JSON-ified List[Dict[str, numpy.ndarray]],
                        i.e. a list where each element is the feed_dict for a
                        single iteration. Other data loader options are
                        ignored when this option is used
  --data-loader-script DATA_LOADER_SCRIPT
                        Path to a Python script that defines a function that
                        loads input data. The function should take no
                        arguments and return a generator or iterable that
                        yields input data (Dict[str, np.ndarray]). When this
                        option is specified, all other data loader arguments
                        are ignored.
  --data-loader-func-name DATA_LOADER_FUNC_NAME
                        When using a data-loader-script, this specifies the
                        name of the function that loads data. Defaults to
                        `load_data`.

Comparator inference:
  Options for running inference via Comparator.run()

  --warm-up NUM         Number of warm-up runs before timing inference
  --use-subprocess      Run runners in isolated subprocesses. Cannot be used
                        with a debugger
  --save-inputs SAVE_INPUTS, --save-input-data SAVE_INPUTS
                        [EXPERIMENTAL] Path to save inference inputs. The
                        inputs (List[Dict[str, numpy.ndarray]]) will be
                        encoded as JSON and saved
  --save-outputs SAVE_RESULTS, --save-results SAVE_RESULTS
                        Path to save results from runners. The results
                        (RunResults) will be encoded as JSON and saved

Comparator comparisons:
  Options for comparing inference results

  --no-shape-check      Disable checking that output shapes match exactly
  --rtol RTOL [RTOL ...], --rel-tol RTOL [RTOL ...]
                        Relative tolerance for output comparison. To specify
                        per-output tolerances, use the format: --rtol
                        [<out_name>:]<rtol>. If no output name is provided,
                        the tolerance is used for any outputs not explicitly
                        specified. For example: --rtol 1e-5 out0:1e-4
                        out1:1e-3. Note that the default tolerance typically
                        works well for FP32, but may be too strict for lower
                        precisions like FP16 or INT8.
  --atol ATOL [ATOL ...], --abs-tol ATOL [ATOL ...]
                        Absolute tolerance for output comparison. To specify
                        per-output tolerances, use the format: --atol
                        [<out_name>:]<atol>. If no output name is provided,
                        the tolerance is used for any outputs not explicitly
                        specified. For example: --atol 1e-5 out0:1e-4
                        out1:1e-3. Note that the default tolerance typically
                        works well for FP32, but may be too strict for lower
                        precisions like FP16 or INT8.
  --validate            Check outputs for NaNs and Infs
  --fail-fast           Fail fast (stop comparing after the first failure)
  --top-k TOP_K [TOP_K ...]
                        [EXPERIMENTAL] Apply Top-K (i.e. find indices of K
                        largest values) to the outputs before comparing
                        them.To specify per-output top-k, use the format:
                        --top-k [<out_name>:]<k>. If no output name is
                        provided, top-k is applied to all outputs. For
                        example: --top-k out:5
  --check-error-stat CHECK_ERROR_STAT [CHECK_ERROR_STAT ...]
                        The error statistic to check. For details on possible
                        values, see the documentation for
                        CompareFunc.simple(). To specify per-output values,
                        use the format: --check-error-stat
                        [<out_name>:]<stat>. If no output name is provided,
                        the value is used for any outputs not explicitly
                        specified. For example: --check-error-stat max
                        out0:mean out1:median
  --load-outputs LOAD_RESULTS [LOAD_RESULTS ...], --load-results LOAD_RESULTS [LOAD_RESULTS ...]
                        Path(s) to load results from runners prior to
                        comparing. Each file should be a JSON-ified RunResults

Runners:
  Options for selecting runners. Zero or more runners may be specified

  --trt                 Run inference using TensorRT
  --trt-legacy          Run inference using Legacy TensorRT Runner. Only
                        supports networks using implicit batch mode
  --tf                  Run inference using TensorFlow
  --onnxrt              Run inference using ONNX Runtime
  --pluginref           Run inference for models containing single TensorRT
                        plugins using a CPU reference implementation
